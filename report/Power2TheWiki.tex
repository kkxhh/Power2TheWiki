\documentclass[11pt,a4paper]{article}

\usepackage[english]{babel}
\usepackage{listings}
\usepackage{url}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{authblk}

\begin{document}

\title{Predicting the translations of red links}
\date{}

\author[ ]{Olga Chernytska}
\author[ ]{Maksym Gontar}
\author[ ]{Kateryna Liubonko}
\author[ ]{Oleksandr Zaytsev}

\affil[ ]{Ukrainian Catholic University}
\affil[ ]{Faculty of Applied Sciences}
\affil[ ]{Lviv, Ukraine}
\affil[ ]{\texttt{\{chernytska,hontar,aloshkina,oleks\}@ucu.edu.ua}}

\maketitle

\begin{abstract}
In this work we propose a technique for matching the red links of Wikipedia to the corresponding articles in another language. We build the graph of all Wikipedia pages and use it to measure similarity between the pages in different languages, one of which exists only as a red link. Such mathing would make the process of filling the gaps in Wikipedia more effective by allowing contributors to translate the existing page that corresponds to a red link rather than writing whole text from scratch.
\end{abstract}

\section{Introduction}

Wikipedia allows its users to create links to the pages that were not yet created. Such links become red and if you click on them, you get redirrected to an empty page and asked to write the missing article. Red links are an important source of information as they let us know what are the gaps in Wikipedia and how important it is to fill them. Every red link is the reference to a non-existing page and the number of such references can serve as a measure of interest that community of contributors has in a certain topic. Filling those gaps is a perfect way of making Wikipedia more complete and interconnected.

A major problem with red links is that because of the way they are stored in database they can not be linked to pages in other languages in a same way as existing pages are linked to their translations. This complicates the contribution process as it is much easier for people to translate and edit the existing page than to write a new one.

We propose a solution for finding the potential translation of the missing pages in other languages. It would allow Wikipedia to automatically match red links to existing pages and make contribution much easier.

\section{Related work}

To our knowledge, there was no work done in this area. In their work The Collaborative Organization of Knowledge, Diomidis Spinellis and Panagiotis Louridas \cite{spinellis} have found that most new articles are created shortly after a corresponding reference to them (red link) is entered into the system and are typically written by different authors from the ones behind the references.

\section{Problem statement}

\begin{enumerate}
	\item Define a distance measure $q: V_{EN} \times V_{UK} \to \mathbb{R}$ that would represent similarity between articles of English ($V_{EN}$) and Ukrainian ($V_{UK}$) Wikipeadia. This measure should be high for articles that are the translations of one another and low for those that are not.
	\item For each red link in English Wikipedia find the closest article in Ukrainian Wikipedia in terms of the defined distance measure. In other words, for each $v^{*} \in V_{EN}^{(red)}$ find
	\[ v = \operatorname*{argmin}_{v \in V_{UK}^{(blue)}} q(v^{*}, v) \]
\end{enumerate}

\section{Data collection and preparation}

We have downloaded the full dumps of English\footnote{\url{https://dumps.wikimedia.org/enwiki/20180620/}} and Ukrainian Wikipedia\footnote{\url{https://dumps.wikimedia.org/ukwiki/20180620/}} articles from 20/06/2018. 

Then we have parsed those articles with regular expressions to get outgoing red and blue links: the link article, text and position in the current article text. There was red links in English Wikipedia and red links Ukrainian Wikipedia. There was blue links in English Wikipedia and blue links Ukrainian Wikipedia. This data is stored in enwiki-20180620-pages-links.csv and ukwiki-20180620-pages-links.csv files. Format of those files is following:\\
id - id of a page\\
link\_id - id of a linked page\\
link\_pos - position of link in a page markup text\\  
link\_pos\_perc - relative position of link in a page markup text, range from 0 (at the beginning of page text) to 1 (at the end of page text)\\  
link\_val - title of a linked page\\  
link\_text - link text, if available\\  
is\_red\_link - boolean, whether a link is red or not\\  

Then we have downloaded Wiki interlanguage link records \footnote{\url{https://dumps.wikimedia.org/ukwiki/20180620/ukwiki-20180620-langlinks.sql.gz}} and parsed out all interlingual links between En and Uk Wiki articles. There was 441928 pairs of Uk-En Wiki articles. This data is stored in 20180620-langlinks\_uk\_en.csv file. Format of this file is following:\\
id\_uk - id of a page in Uk Wiki\\
id\_en - id of a linked page in En Wiki\\

From dumps we collected data about pages aliases (redirects) in En and Uk Wiki. The alias page is the page user can come upon by searching the article not by it's original name, but by it's alias name, then user is redirected to the original page. Alias data is important for our task, since links may lead not to the original article, but to it's redirect page. This data is stored in set of files:\\
\\
enwiki-20180620-id\_alias\_title\_alias.csv, ukwiki-20180620-id\_alias\_title\_alias.csv with format:\\
id\_alias - id of alias page\\
title\_alias - title of alias page\\
\\
enwiki-20180620-id\_alias\_id\_orig.csv, ukwiki-20180620-id\_alias\_id\_orig.csv with format:\\
id\_alias - id of alias article\\
id\_orig - id of original article\\
\\
enwiki-20180620-id\_orig\_title\_alias.csv, ukwiki-20180620-id\_orig\_title\_alias.csv with format:\\
id\_orig - id of original article\\
title\_alias - title of alias pages\\

Also we composed a list of all pages in En and Uk Wiki. There were 5669865 in En and 581098 articles Uk Wiki. This data is stored in enwiki-20180620-id\_name.csv, ukwiki-20180620-id\_name.csv files. Format of those files is following:\\ 
id - id of a page\\
title - title of a page\\
length - length of a page markup text\\

Besides that, we did a statistical analysis for red links, we calculated how many times each red link was used and saved results in the enwiki-20180620-red\_name\_count.csv, ukwiki-20180620-red\_name\_count.csv files with format:\\
link\_title - title of a red link\\
in\_count - number of times it was used\\
\\
Also we calculated how many red links there are with a certain number of use, and stored results into the enwiki-20180620-red\_count\_by\_count.csv, ukwiki-20180620-red\_count\_by\_count.csv files with format:\\
count - number of red link usage\\
in\_count - number of this count case\\

For example, in En Wiki red link was used only once for 4354094, and twice for 811612 times.\\

For every eng red link in the matrix we calculate similarity (using similarity metrics selected on the previous step) to those ukr articles that do not have eng version. The most similar ukr article is the one that correeponds to this red link. Possibly, we will find several ukr articles with the same similarities, so add them as candidates for further preprocessing.

\section{Baseline assumption}

\[ p(a)p(b)p(c) < p(a)p(b) \]

\bibliographystyle{plain}
\bibliography{Power2TheWiki}

\end{document}